default:
  tags:
    # Use an account that maps will switch users depending on the repo it
    # runs on
    - bb5_map
  interruptible: true

.gitlab_pipelines_variables:
  variables:
    # Default values for Slurm
    SALLOC_ACCOUNT: proj12
    SBATCH_ACCOUNT: proj12
    SLURM_ACCOUNT: proj12
    SALLOC_PARTITION: prod
    SBATCH_PARTITION: prod
    SLURM_PARTITION: prod
    # We never use the runner's native git checkout ability. We use `git clone`
    # explicitly to get Spack, and then Spack itself clones any projects we want
    # to build.
    GIT_STRATEGY: none
    # Make ${CI_BUILDS_DIR} points to a top-level pipeline-global directory.
    bb5_build_dir: pipeline
    # Make the top-level jobs launched by the GitLab CI use the same account
    bb5_account: proj12
    # Shared input directory for HPC integration tests.
    DATADIR: /gpfs/bbp.cscs.ch/project/proj12/jenkins

# Set up a clone of Spack for use in this pipeline and export a variable called
# SPACK_ROOT that points to it.
.spack_setup:
  stage: .pre
  extends: .gitlab_pipelines_variables
  variables:
    # Put the clone of spack at {pipeline_id}/{spack_setup_jobid}/spack, there
    # is no need to include the path to the project running the pipeline (e.g.
    # hpc/coreneuron). It makes sense to include the job ID because it means
    # that the spack_setup job can be retried in case of a transient failure.
    GIT_CLONE_PATH: ${CI_BUILDS_DIR}/${CI_JOB_ID}
    # What version of BlueBrain/spack to use. You can override this if need be.
    SPACK_BRANCH: develop
    # Which URL to use to check out spack
    SPACK_URL: https://github.com/BlueBrain/spack.git
    # This is a trivial setup job, it doesn't need many resources
    bb5_memory: 512M
    bb5_cpus_per_task: 1
    bb5_duration: "10:00"
  before_script:
    # Make sure this file always exists, even if we are not doing any GitHub PR
    # parsing magic.
    - touch spack_clone_variables.env
    # If we are triggered by a PR on GitHub, fetch the body of the PR description
    - if [[ "${PARSE_GITHUB_PR_DESCRIPTIONS}" == "true" && "${CI_PIPELINE_SOURCE}" == "external_pull_request_event" ]]; then
    # Write a small Python script that queries the GitHub API and prints out a
    # series of variable declarations. See the README for documentation of the
    # required content of the GitHub PR description. The expressions in the PR
    # description are parsed into variables of the form:
    #  SPACK_PACKAGE_REF_{package_name}={ref_type}="{ref}"
    # where `ref_type` is one of `branch`, `tag` and `ref`, for example
    #  SPACK_PACKAGE_REF_neuron=branch="some/feature_branch"
    - |
      cat > parse_description.py << END_SCRIPT
      from __future__ import print_function
      import os
      import re
      import requests
      pr_info = requests.get("https://api.github.com/repos/{}/pulls/{}".format(
                              os.environ['CI_EXTERNAL_PULL_REQUEST_TARGET_REPOSITORY'],
                              os.environ['CI_EXTERNAL_PULL_REQUEST_IID']),
                             headers={'Accept': 'application/vnd.github.v3+json'})
      pr_body = pr_info.json()["body"]
      # match something like NEURON_BRANCH=foo/bar
      pat = re.compile('^([A-Z0-9_]+)_([A-Z]+)=([A-Z0-9\-\_\/]+)$', re.IGNORECASE)
      def parse_term(m):
        ref_type = m.group(2).lower()
        if ref_type not in {'branch', 'tag', 'ref'}: return
        print('SPACK_PACKAGE_REF_' + m.group(1).lower() + '=' + ref_type + '="' + m.group(3) + '"')
      if pr_body is not None:
        for pr_body_line in pr_body.splitlines():
          if not pr_body_line.startswith('CI_BRANCHES:'): continue
          for config_term in pr_body_line[12:].split(','):
            pat.sub(parse_term, config_term)
      END_SCRIPT
    # GitLab's UI collapses the multi-line command above so this aids debugging
    - cat parse_description.py
    # Save the variables we parsed out of the GitHub PR body for later jobs.
    - python parse_description.py > spack_clone_variables.env
    - cat spack_clone_variables.env
    # endif that we're triggered by a PR on GitHub
    - fi
  script:
    - echo Using ${PWD} as a working directory common to the whole pipeline...
    # Figure out what branch of Spack to checkout. If this was steered by the PR
    # description then there will be an entry of the form:
    #   SPACK_PACKGE_REF_spack=(tag|branch|commit)="..."
    # in the file `spack_clone_variables.env`. For now, we only support the case
    # that it is a branch specification. That's easier and should usually be
    # sufficient. If a matching line is found, set ${SPACK_BRANCH_PARSED} to the
    # branch name. Note that doing `. spack_clone_variables.env` to read the
    # values into the environment is a bad idea, because Bash and GitLab have
    # different ideas about how to handle the quotes.
    - SPACK_BRANCH_PARSED=$(sed -n -e "s/^SPACK_PACKAGE_REF_spack=branch=\(['\"]\)\([^\1]\+\)\1/\2/p" spack_clone_variables.env)
    # If ${SPACK_PACKAGE_REF_spack} is not set, use ${SPACK_BRANCH}.
    - SPACK_BRANCH=${SPACK_BRANCH_PARSED:-$SPACK_BRANCH}
    - echo Checking out the ${SPACK_BRANCH} of Spack...
    - git clone --depth 1 --single-branch --branch ${SPACK_BRANCH} ${SPACK_URL} spack
    - cp /gpfs/bbp.cscs.ch/ssd/apps/hpc/jenkins/config/*.yaml spack/etc/spack
    - SPACK_ROOT=${PWD}/spack
    - echo "SPACK_ROOT=${SPACK_ROOT}" >> spack_clone_variables.env
    - . ${SPACK_ROOT}/share/spack/setup-env.sh
  artifacts:
    when: always
    paths: [spack_clone_variables.env]
    reports:
      dotenv: spack_clone_variables.env

# Augmented version of .spack_setup that additionally enables ccache in the
# Spack configuration and exports a variable SPACK_USE_CCACHE=true for use in
# later stages.
.spack_setup_ccache:
  extends: .spack_setup
  script:
    - !reference [.spack_setup, script]
    # Enable ccache
    - spack config --scope site add "config:ccache:true"
    # Dump the configuration for debugging/reference purposes.
    - spack config get config
    # In principle later stages could use some kind of
    # `spack config get config | grep ccache` construction, but this is simpler
    - echo "SPACK_USE_CCACHE=true" >> spack_clone_variables.env

.spack_build:
  extends: .gitlab_pipelines_variables
  stage: build
  variables:
    # Work in a directory unique to this job.
    GIT_CLONE_PATH: ${CI_BUILDS_DIR}/${CI_JOB_ID}
    # By default we update the `develop` version of the Spack recipe to point
    # to this commit ("the commit the pipeline is being run on"). This variable
    # can be overriden, also with tag="..." or branch="...".
    SPACK_PACKAGE_REF: commit="${CI_COMMIT_SHA}"
  before_script:
    # Change the staging directory to something specific to this GitLab CI job.
    # This stops Spack from cleaning up previous build directories, which
    # causes errors if you try to run "test:library" in parallel with
    # "build:application_that_depends_on_library".
    - SPACK_BUILD="${PWD}/spack-build"
    # Dump the environment for debugging purposes
    - env -0 | sort -z | xargs -0 -L 1 echo > initial_environment.env
    # SPACK_ROOT is passed in by the dotenv artifacts of a previous job.
    - . ${SPACK_ROOT}/share/spack/setup-env.sh
    # Tell Spack which working directory to use in this job. It would be nice
    # if this first part could be done with `spack config add`...
    - SPACK_CONFIG=${PWD}/spack-config
    - mkdir ${SPACK_CONFIG}
    - |
      cat > ${SPACK_CONFIG}/config.yaml << END_SCRIPT
      config:
        build_stage:
        - ${SPACK_BUILD}
        source_cache: ${PWD}/spack-source-cache
      END_SCRIPT
    - spack --config-scope ${SPACK_CONFIG} config blame config
    - if [ "${SPACK_EXPORT_SPECS}" ]; then
    - spack export --scope=command_line --module tcl ${SPACK_EXPORT_SPECS} > ${SPACK_CONFIG}/packages.yaml
    - cat ${SPACK_CONFIG}/packages.yaml
    - fi
    # Tell Git how to re-write BBP GitLab URLs to use a token instead of SSH
    - export XDG_CONFIG_HOME=${PWD}/local_config
    - mkdir -p "${XDG_CONFIG_HOME}/git"
    - echo -e "[url \"https://gitlab-ci-token:${CI_JOB_TOKEN}@bbpgitlab.epfl.ch/\"]\n  insteadOf = git@bbpgitlab.epfl.ch:" > "${XDG_CONFIG_HOME}/git/config"
    - cat "${XDG_CONFIG_HOME}/git/config"
    # Figure out what the develop version of this package should refer to. If
    # this was steered by the PR description then the variable
    # ${SPACK_PACKAGE_REF_${SPACK_PACKAGE}} should be set to a valid value and
    # this should take precedence. Otherwise, use ${SPACK_PACKAGE_REF}.
    - REF_OVERRIDE_NAME=SPACK_PACKAGE_REF_${SPACK_PACKAGE}
    - PACKAGE_REF=${!REF_OVERRIDE_NAME:-$SPACK_PACKAGE_REF}
    # Modify the Spack recipe so the develop version points at ${SPACK_PACKAGE_REF}
    - SPACK_PACKAGE_FILE="$(spack location -p ${SPACK_PACKAGE})/package.py"
    # Remove any commit/tag/branch identifier from the line containing the
    # develop version and substitute in our desired commit hash. Don't
    # change anything if ${PACKAGE_REF} is unset or an empty string.
    - if [ "${PACKAGE_REF}" ]; then
    - echo New ref for ${SPACK_PACKAGE} is ${PACKAGE_REF}
    # Note that [^\2] doesn't work, backreferences are not expanded there, so
    # we have to use [^\"'].
    - sed -i -e "/^\s*version\s*(\s*\(['\"]\)develop\1/ { s/,\s*\(commit\|tag\|branch\)=\([\"']\)[^\"']\+\2//g; s#)#, ${PACKAGE_REF})#; }" "${SPACK_PACKAGE_FILE}"
    - (cd $(dirname "${SPACK_PACKAGE_FILE}") && git diff "${SPACK_PACKAGE_FILE}")
    - fi
    # Get the hash of the version we're about to install. First, construct the
    # full spec. This includes ${SPACK_PACKAGE_COMPILER} and ${SPACK_PACKAGE_DEPENDENCIES} if
    # they are not empty.
    - SPACK_FULL_SPEC="${SPACK_PACKAGE}@develop${SPACK_PACKAGE_COMPILER:+%}${SPACK_PACKAGE_COMPILER}${SPACK_PACKAGE_SPEC}${SPACK_PACKAGE_DEPENDENCIES:+ }${SPACK_PACKAGE_DEPENDENCIES}"
    - echo "Preparing to install ${SPACK_FULL_SPEC}"
    # Then extract the hash
    - JSON_SPEC=$(spack --config-scope ${SPACK_CONFIG} spec --json ${SPACK_FULL_SPEC})
    - SPACK_INSTALLED_HASH=$(echo "${JSON_SPEC}" | python -c "import json, sys; print(json.loads(sys.stdin.read())[\"spec\"][0][\"${SPACK_PACKAGE}\"][\"hash\"])")
    - echo "Determined its hash will be ${SPACK_INSTALLED_HASH}"
    # Construct the directories Spack is going to use to build the package.
    # For a CMake project the source directory will be:
    #   ${SPACK_STAGE_DIR}/spack-src
    # and the build directoy will by default be
    #   ${SPACK_STAGE_DIR}/spack-build-{short_hash}
    # TODO: to improve ccache support, drop the {short_hash} part. This needs a
    # change to `cmake.py` in Spack.
    - SPACK_STAGE_DIR=${SPACK_BUILD}/spack-stage-${SPACK_PACKAGE}-develop-${SPACK_INSTALLED_HASH}
    - SPACK_BUILD_DIR=${SPACK_STAGE_DIR}/spack-build-${SPACK_INSTALLED_HASH:0:7}
    - SPACK_SOURCE_DIR=${SPACK_STAGE_DIR}/spack-src
    # ccache-specific setup, only if it was enabled
    - if [ ${SPACK_USE_CCACHE+x} ]; then
    # Load a more modern ccache version.
    - module load unstable ccache
    # Tell ccache to use paths relative to this directory, to avoid polluting
    # the cache with pipeline- and job-specific paths.
    - export CCACHE_BASEDIR=${SPACK_STAGE_DIR}
    - export CCACHE_DIR=${CI_PROJECT_DIR}/ccache
    # Default is 5G. There is no automatic cleaning of the caches on the GitLab
    # side, so this is a good way of stopping things from growing too much.
    - export CCACHE_MAXSIZE=512M
    # For debugging cache misses.
    # - export CCACHE_DEBUG=true
    # - export CCACHE_DEBUGDIR=${CI_PROJECT_DIR}/ccache_debug/
    # We can't assume there was a valid GitLab cache, so create empty
    # directories if needed.
    - mkdir -p ${CCACHE_BASEDIR} # ${CCACHE_DEBUGDIR}
    # Zero the statistics.
    - ccache --zero-stats
    - ccache --show-stats --verbose
    - fi
    # end ccache-specific setup
  script:
    # Spack needs a recent Git version.
    # The system GCC on BB5 is very old (4.8), and some recipes (coreneuron...)
    # assume that `which gcc` will find a GCC version that can be used as a
    # CUDA/NVCC host compiler.
    - module load unstable git gcc
    # *Un*install this spec first. This can be helpful when resubmitting failed
    # jobs, for example if the GitLab job finalisation fails then the package
    # might have been successfully installed to the pipeline's Spack tree. Also
    # include `--dependents` so that if we are installing `lib` and then `app`
    # that depends on it then re-trying to build `lib` after `app` has been
    # installed will uninstall both `lib` and `app` instead of neither.
    - spack --config-scope ${SPACK_CONFIG} uninstall -y --dependents /${SPACK_INSTALLED_HASH} || true
    # Show what we're going to do, ignore the duplication with `spack spec`
    # above for now
    - spack --config-scope ${SPACK_CONFIG} spec -Il ${SPACK_FULL_SPEC}
    # Install that new version. Use an absolute path for install.xml so it ends
    # up in the original job working directory even if that's no longer the
    # working directory.
    - spack ${SPACK_INSTALL_EXTRA_FLAGS} --config-scope ${SPACK_CONFIG} install -j${SLURM_JOB_CPUS_PER_NODE} --log-format=junit --log-file=${CI_PROJECT_DIR}/install.xml --keep-stage ${SPACK_FULL_SPEC}
    # Report ccache statistics if ccache was enabled.
    - if [ ${SPACK_USE_CCACHE+x} ]; then
    - ccache --cleanup
    - ccache --show-stats --verbose
    - fi
    # Copy some files to the original working directory that artifacts are
    # saved from.  Make sure that optional files exist.
    - touch ${SPACK_STAGE_DIR}/spack-configure-args.txt
    - cp ${SPACK_STAGE_DIR}/spack-{build-env,build-out,configure-args}.txt ${CI_PROJECT_DIR}/
    # NOTE changed behaviour. Before we would append ^/new_hash to the
    # SPACK_PACKAGE_DEPENDENCIES variable. This produced a rather long spec for
    # the later stages of a chain of build jobs, and also included any extra
    # specs that were specified manually along the way, i.e. for A <- B <- C
    #   ^dep_of_A+variant^/hash_of_A^dep_of_B%gcc^/hash_of_B
    # Now we just include ^/hash_of_B in SPACK_PACKAGE_DEPENDENCIES after
    # building B, and trust that that encodes all relevant constraints.
    - SPACK_PACKAGE_DEPENDENCIES=^/${SPACK_INSTALLED_HASH}
    # Overwrite the spack_build_info.env file, otherwise chains of dependent
    # build jobs will duplicate these values.
    - echo "SPACK_BUILD_DIR=${SPACK_BUILD_DIR}" > ${CI_PROJECT_DIR}/spack_build_info.env
    - echo "SPACK_FULL_SPEC=${SPACK_FULL_SPEC}" >> ${CI_PROJECT_DIR}/spack_build_info.env
    - echo "SPACK_SOURCE_DIR=${SPACK_SOURCE_DIR}" >> ${CI_PROJECT_DIR}/spack_build_info.env
    - echo "SPACK_INSTALLED_HASH=${SPACK_INSTALLED_HASH}" >> ${CI_PROJECT_DIR}/spack_build_info.env
    - echo "SPACK_PACKAGE_DEPENDENCIES=${SPACK_PACKAGE_DEPENDENCIES}" >> ${CI_PROJECT_DIR}/spack_build_info.env
    # Return a meaningful status code by inspecting install.xml. This only
    # loads modules in a subshell so it doesn't pollute the environment. The
    # snippet counts the number of <failure> tags in the XML.
    - num_failures=$(module load unstable python-dev; python -c "from lxml import etree; xml = etree.parse('${CI_PROJECT_DIR}/install.xml'); print(sum(1 for _ in xml.getroot().iter('failure')) + sum(1 for _ in xml.getroot().iter('error')))")
    - echo "Returning the number of failed builds, ${num_failures}"
    - exit ${num_failures}
  needs: ["spack_setup"]
  artifacts:
    when: always
    paths:
      - install.xml
      - spack_build_info.env
      - spack_clone_variables.env
      - initial_environment.env
      - spack-build-env.txt
      - spack-build-out.txt
      - spack-configure-args.txt
    reports:
      junit: install.xml
      dotenv:
        - spack_build_info.env
        - spack_clone_variables.env
  cache:
    key: ${SPACK_PACKAGE}-${SPACK_PACKAGE_COMPILER}
    paths:
      - ccache/
    policy: pull-push

.spack_test:
  extends: .gitlab_pipelines_variables
  stage: test
  variables:
    # Just running tests, no need to check anything out
    GIT_STRATEGY: none
    # Make CI_BUILDS_DIR be the top-level pipeline-global directory
    bb5_build_dir: pipeline
    # Run tests with many 1-core tasks instead of 1 many-core task, as this
    # means that naive `mpirun -n ...` should work.
    bb5_ntasks: 8
    bb5_cpus_per_task: 1
    GIT_CLONE_PATH: ${CI_BUILDS_DIR}/${CI_JOB_ID}
  before_script:
    # Dump the environment for debugging purposes
    - env -0 | sort -z | xargs -0 -L 1 echo > initial_environment.env
    # Tell CTest to use the available slots. This may not quite be correct if
    # the tasks are allocated across several nodes.
    - export CTEST_PARALLEL_LEVEL=${SLURM_TASKS_PER_NODE}
    # Load the Spack installation that knows about the package to be tested.
    - . ${SPACK_ROOT}/share/spack/setup-env.sh
  script:
    - spack load ${SPACK_FULL_SPEC}
    - sh ${SPACK_SOURCE_DIR}/.ci/test_${CI_JOB_NAME}.sh
  artifacts:
    when: always
    paths:
      - initial_environment.env

.ctest:
  extends: .spack_test
  script:
    # Change to the build directory of the package being tested. This is
    # somewhere under the working directory of a previous job in the pipeline.
    - cd ${SPACK_BUILD_DIR}
    # Yuck, but otherwise boost unit tests output colour codes as part of
    # ctest -VV and the XML translation fails.
    - export BOOST_TEST_COLOR_OUTPUT=no
    # Make sure we return a helpful exit code.
    - i_am_a_failure=0
    # --output-on-failure should stop the output from parallel jobs being
    # interleaved. The full logfile including output from all jobs is uploaded
    # as an artefact, so we shouldn't lose any information.
    - spack build-env ${SPACK_FULL_SPEC} -- ctest --output-on-failure -T Test || i_am_a_failure=1
    # Save the Testing/ directory as job artifacts
    - cp -r Testing/ ${CI_PROJECT_DIR}/
    # Make an XML report file the GitLab UI can display
    - module load unstable unit-test-translator
    - cmake2junit > ${CI_PROJECT_DIR}/ctest.xml
    - exit ${i_am_a_failure}
  artifacts:
    when: always
    paths:
      # Ugly to re-declare this from .spack_test, but oh well.
      - initial_environment.env
      - Testing/
    reports:
      junit: ctest.xml
