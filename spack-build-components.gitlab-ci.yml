default:
  tags:
    # Use an account that maps will switch users depending on the repo it
    # runs on
    - bb5_map
  interruptible: true

include: 'bbp-gitlab-access.yml'

.gitlab_pipelines_variables:
  variables:
    # These two variables try to make `mpiexec -n <N>` do something sensible
    # when used inside a CI job that runs on BB5. Because of the way the BB5
    # runner is set up, the CI script (i.e. the lines you put inside the
    # before_script and script YAML blocks) is run inside the job allocation
    # using srun and occupies all the resources. If we want to spawn another
    # job from that CI script, we have to use --overlap so that it is allowed
    # to run on those already-occupied occupied resources. This means that jobs
    # spawned from the CI script are also allocated all of the resources in the
    # job allocation. If the spawned child job is an MPI application then by
    # default hpe-mpi will pin the MPI ranks to cores starting from the
    # beginning of the allocated resources. Because all child jobs are
    # allocated all of the resources, this means that the child ranks are all
    # pinned to the same cores. Setting MPI_DSM_DISTRIBUTE=0 disables this
    # pinning and lets the OS scheduler spread the spawned jobs over the
    # available cores. This might not work well if the allocated resources span
    # multiple nodes.
    SLURM_OVERLAP: 1
    MPI_DSM_DISTRIBUTE: 0
    # Default values for Slurm
    SALLOC_ACCOUNT: proj12
    SBATCH_ACCOUNT: proj12
    SLURM_ACCOUNT: proj12
    SALLOC_PARTITION: prod
    SBATCH_PARTITION: prod
    SLURM_PARTITION: prod
    # We never use the runner's native git checkout ability. We use `git clone`
    # explicitly to get Spack, and then Spack itself clones any projects we want
    # to build.
    GIT_STRATEGY: none
    # ${CI_BUILDS_DIR} will point to some {...}/{pipelineid} directory.
    bb5_build_dir: pipeline
    # Do not put the name of the repository in the working directory
    GIT_CLONE_PATH: ${CI_BUILDS_DIR}/J${CI_JOB_ID}
    # Shared input directory for HPC integration tests.
    DATADIR: /gpfs/bbp.cscs.ch/project/proj12/jenkins

# Set up a clone of Spack for use in this pipeline and export a variable called
# SPACK_ROOT that points to it.
.spack_setup:
  stage: .pre
  extends: .gitlab_pipelines_variables
  variables:
    # What version of BlueBrain/spack to use. You can override this if need be.
    SPACK_BRANCH: configure-git-versions
    SPACK_DEPLOYMENT_SUFFIX: pulls/1900
    # Which URL to use to check out spack
    SPACK_URL: https://github.com/BlueBrain/spack.git
  # before_script handles some extra setup of the environment. It fetches a
  # file containing extra environment variables, either from:
  #  - the description of an associated GitHub PR, or
  #  - the artifacts of a previous job, via the SPACK_ENV_FILE_URL variable.
  # in order that these variables take predence over global defaults, a list of
  # projects that have PROJECT_{BRANCH,COMMIT,TAG} variables in the file is
  # generated, and any corresponding PROJECT_{BRANCH,COMMIT,TAG} variables that
  # are already set in the environment are unset.
  before_script:
    - if [[ -n "${SPACK_ENV_FILE_URL}" && "${PARSE_GITHUB_PR_DESCRIPTIONS,,}" == "true" ]]; then
    - echo "Only one of SPACK_ENV_FILE_URL and PARSE_GITHUB_PR_DESCRIPTIONS=true can be set."
    - exit 1
    - elif [[ -n "${SPACK_ENV_FILE_URL}" ]]; then
    - echo SPACK_ENV_FILE_URL was set to ${SPACK_ENV_FILE_URL}, fetching it...
    - curl --location --output input_variables.env "${SPACK_ENV_FILE_URL}?job_token=${CI_JOB_TOKEN}"
    - elif [[ "${PARSE_GITHUB_PR_DESCRIPTIONS,,}" == "true" && "${CI_PIPELINE_SOURCE}" == "external_pull_request_event" ]]; then
    # If we are triggered by a PR on GitHub, fetch the body of the PR description
    # Write a small Python script that queries the GitHub API and prints out a
    # series of variable declarations. See the README for documentation of the
    # required content of the GitHub PR description. The expressions in the PR
    # description are parsed into variables of the form:
    #  {package_name_upper}_{ref_type_upper}={ref}
    # where `ref_type` is one of `branch`, `tag` and `ref`, for example
    #  NEURON_BRANCH=some/feature-branch
    - |
      cat > parse_description.py << END_SCRIPT
      import os
      import re
      import requests
      pr_info = requests.get("https://api.github.com/repos/{}/pulls/{}".format(
                              os.environ['CI_EXTERNAL_PULL_REQUEST_TARGET_REPOSITORY'],
                              os.environ['CI_EXTERNAL_PULL_REQUEST_IID']),
                             headers={'Accept': 'application/vnd.github.v3+json'})
      pr_body = pr_info.json()["body"]
      # match something like NEURON_BRANCH=foo/bar
      pat = re.compile('^([A-Z0-9_]+)_([A-Z]+)=([A-Z0-9\-\_\/\+\.]+)$', re.IGNORECASE)
      def parse_term(m):
        ref_type = m.group(2).lower()
        if ref_type not in {'branch', 'tag', 'ref'}: return
        print(m.group(1).upper() + '_' + ref_type.upper() + '=' + m.group(3))
      if pr_body is not None:
        for pr_body_line in pr_body.splitlines():
          if not pr_body_line.startswith('CI_BRANCHES:'): continue
          for config_term in pr_body_line[12:].split(','):
            pat.sub(parse_term, config_term)
      END_SCRIPT
    # GitLab's UI collapses the multi-line command above so this aids debugging
    - cat parse_description.py
    # Save the variables we parsed out of the GitHub PR body for later jobs.
    - (module load unstable python-dev; python parse_description.py) > input_variables.env
    - else
    - touch input_variables.env # Make sure this file exists in any case.
    - fi # end of blocks that populate input_variables.env
    # input_variables.env now exists; get a list of project names that it
    # contains _BRANCH, _COMMIT or _TAG variables for and unset any _BRANCH,
    # _COMMIT or _TAG variables for those projects that are already set in the
    # environment.
    - cat input_variables.env
    - |
      for var_to_unset in $(sed 's/^\(.*\?\)_\(BRANCH\|COMMIT\|TAG\)=.*$/\1_BRANCH\n\1_COMMIT\n\1_TAG/' input_variables.env); do
      if [[ -n "${!var_to_unset}" ]]; then echo "Unsetting ${var_to_unset}"; unset ${var_to_unset}; fi
      done
    # Load the new variables into the environment
    - set -o allexport
    - . input_variables.env
    - set +o allexport
  script:
    # Prefer the 2022 deployment modules.
    - unset MODULEPATH
    - . /gpfs/bbp.cscs.ch/ssd/apps/bsd/${SPACK_DEPLOYMENT_SUFFIX}/config/modules.sh
    - echo "MODULEPATH=${MODULEPATH}" > spack_clone_variables.env
    - echo Preparing to clone Spack into ${PWD}
    - if [[ -z "${SPACK_BRANCH}" && ( -n "${SPACK_COMMIT}" || -n "${SPACK_TAG}" ) ]]; then
    - echo "Spack ref must be a branch (BRANCH=${SPACK_BRANCH}, COMMIT=${SPACK_COMMIT}, TAG=${SPACK_TAG})"
    - fi
    - echo Checking out the ${SPACK_BRANCH} of Spack...
    - module load unstable git
    - git clone -c feature.manyFiles=true --depth 1 --single-branch --branch ${SPACK_BRANCH} ${SPACK_URL} spack
    - export SPACK_ROOT=${PWD}/spack
    - export SPACK_USER_CACHE_PATH="${CI_BUILDS_DIR}"
    - export SPACK_SYSTEM_CONFIG_PATH="/gpfs/bbp.cscs.ch/ssd/apps/bsd/${SPACK_DEPLOYMENT_SUFFIX}/config"
    - echo "SPACK_ROOT=${SPACK_ROOT}" >> spack_clone_variables.env
    - echo "SPACK_USER_CACHE_PATH=${SPACK_USER_CACHE_PATH}" >> spack_clone_variables.env
    - echo "SPACK_SYSTEM_CONFIG_PATH=${SPACK_SYSTEM_CONFIG_PATH}" >> spack_clone_variables.env
    - . ${SPACK_ROOT}/share/spack/setup-env.sh
    # Tell Git how to re-write BBP GitLab URLs to use a token instead of SSH
    - !reference [.bbp_gitlab_access, script]
    # Modify any and all Spack recipes based on the PROJECT_{BRANCH,COMMIT,TAG}
    # variables. If we can't figure out what to do, error out immediately. A
    # few variables are explicitly ignored: CI_COMMIT_BRANCH and
    # CI_DEFAULT_BRANCH are set by GitLab, and GITLAB_PIPELINES_BRANCH and
    # SPACK_BRANCH are special.
    # If, for example, NEURON_BRANCH=master is set then the file
    # commit-mapping.env will contain a line NEURON_COMMIT=sha_of_master_now.
    # Note that empty variables will be ignored. If set,
    # SPACK_SETUP_IGNORE_PACKAGE_VARIABLES is a whitespace-separated list of
    # PROJECT names that should be ignored. Variables starting with CUSTOM_ENV_
    # seem to be set if the `bb5` tag is used, and are duplicates that should
    # be ignored. This sed command deletes lines starting with CUSTOM_ENV_ and
    # then prints lines starting with [^=]\+_\(BRANCH\|COMMIT\|TAG\)=.\+
    - env -0 | sed -nz '/^CUSTOM_ENV_/d;/^[^=]\+_\(BRANCH\|COMMIT\|TAG\)=.\+/p' | xargs -0t spack configure-pipeline --ignore-packages CI_BUILD CI_COMMIT CI_DEFAULT GITLAB_PIPELINES SPACK ${SPACK_SETUP_IGNORE_PACKAGE_VARIABLES} --write-commit-file=commit-mapping.env
    # Show what we did for debugging purposes.
    - (cd "${SPACK_ROOT}" && git diff)
    - spack config blame packages
    - cat commit-mapping.env # Exporting the following variables for subsequent jobs
    # The logic here is that commit-mapping.env should include all of the
    # branches/commits/external information needed to produce the same Spack,
    # but it should not refer to any paths in the pipeline working directory.
    # spack_clone_variables.env additionally includes paths such as SPACK_ROOT.
    - echo "SPACK_BRANCH=${SPACK_BRANCH}" >> commit-mapping.env
    - echo "SPACK_DEPLOYMENT_SUFFIX=${SPACK_DEPLOYMENT_SUFFIX}" >> commit-mapping.env
    - cat commit-mapping.env >> spack_clone_variables.env
    # Trigger bootstrapping and so on, check everything looks OK
    - spack spec -IL ninja
    # This makes it easy to tell child pipelines (using SPACK_ENV_FILE_URL) to
    # load this commit-mapping.env file.
    - echo "SPACK_SETUP_COMMIT_MAPPING_URL=${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/jobs/${CI_JOB_ID}/artifacts/commit-mapping.env" >> spack_clone_variables.env
  artifacts:
    when: always
    paths: [commit-mapping.env, input_variables.env, spack_clone_variables.env]
    reports:
      dotenv: spack_clone_variables.env

# Augmented version of .spack_setup that additionally enables ccache in the
# Spack configuration and exports a variable SPACK_USE_CCACHE=true for use in
# later stages.
.spack_setup_ccache:
  extends: .spack_setup
  script:
    - !reference [.spack_setup, script]
    # Enable ccache
    - spack config --scope site add "config:ccache:true"
    # In principle later stages could use some kind of
    # `spack config get config | grep ccache` construction, but this is simpler
    - echo "SPACK_USE_CCACHE=true" >> spack_clone_variables.env

.spack_build:
  extends: .gitlab_pipelines_variables
  stage: build
  before_script:
    # Change the staging directory to something specific to this GitLab CI job.
    # This stops Spack from cleaning up previous build directories, which
    # causes errors if you try to run "test:library" in parallel with
    # "build:application_that_depends_on_library".
    - SPACK_BUILD="${PWD}/spack-build"
    # Dump the environment for debugging purposes
    - env -0 | sort -z | xargs -0 -L 1 echo > initial_environment.env
    # SPACK_ROOT is passed in by the dotenv artifacts of a previous job.
    - . ${SPACK_ROOT}/share/spack/setup-env.sh
    # Tell Spack which working directory to use in this job. It would be nice
    # if this first part could be done with `spack config add`...
    - export SPACK_USER_CONFIG_PATH=${PWD}/spack-config
    - mkdir ${SPACK_USER_CONFIG_PATH}
    - |
      cat > ${SPACK_USER_CONFIG_PATH}/config.yaml << END_SCRIPT
      config:
        build_stage::
        - ${SPACK_BUILD}
        source_cache: ${PWD}/spack-source-cache
      END_SCRIPT
    - spack config add concretizer:reuse:false
    - spack config blame config
    - spack config blame concretizer
    # Tell Git how to re-write BBP GitLab URLs to use a token instead of SSH
    - !reference [.bbp_gitlab_access, script]
    # First, construct the full spec. This includes ${SPACK_PACKAGE_COMPILER} and
    # ${SPACK_PACKAGE_DEPENDENCIES} if they are not empty.
    - SPACK_FULL_SPEC="${SPACK_PACKAGE}${SPACK_PACKAGE_COMPILER:+%}${SPACK_PACKAGE_COMPILER} ${SPACK_PACKAGE_SPEC} ${SPACK_PACKAGE_DEPENDENCIES} ${SPACK_PACKAGE_DEPENDENCY_ON_PREVIOUS_JOB}"
    - echo "Preparing to install ${SPACK_FULL_SPEC}"
    # ccache-specific setup, only if it was enabled
    - if [ ${SPACK_USE_CCACHE+x} ]; then
    # Load a more modern ccache version.
    - module load unstable ccache
    # Tell ccache to use paths relative to this directory, to avoid polluting
    # the cache with pipeline- and job-specific paths.
    - export CCACHE_BASEDIR=$(realpath -P ${CI_BUILDS_DIR})
    - echo CCACHE_BASEDIR=${CCACHE_BASEDIR}
    # Default is 5G. There is no automatic cleaning of the caches on the GitLab
    # side, so this is a good way of stopping things from growing too much.
    - export CCACHE_MAXSIZE=512M
    # For debugging cache misses.
    # - export CCACHE_DEBUG=true
    # - export CCACHE_DEBUGDIR=${CI_PROJECT_DIR}/ccache_debug/
    # We can't assume there was a valid GitLab cache, so create empty
    # directories if needed.
    - export CCACHE_DIR="${TMPDIR}/ccache"
    - mkdir -p ${CCACHE_DIR}
    - if [ -f ${CI_PROJECT_DIR}/ccache.tar ]; then
    # There was a valid cache from GitLab
    - tar -C "${CCACHE_DIR}" -xf "${CI_PROJECT_DIR}/ccache.tar"
    - fi
    # Zero the statistics.
    - ccache --zero-stats
    - ccache --show-stats --verbose
    - fi
    # end ccache-specific setup
  script:
    # Spack needs a recent Git version. It possible to load additional
    # modules through the SPACK_EXTRA_MODULES environment variable.
    - module load unstable git ${SPACK_EXTRA_MODULES}
    # Show what we're going to do, ignore the duplication with `spack spec`
    # above for now
    - spack spec -Il ${SPACK_FULL_SPEC}
    # Install that new version. Use an absolute path for install.xml so it ends
    # up in the original job working directory even if that's no longer the
    # working directory.
    - set -o pipefail
    - (spack ${SPACK_INSTALL_EXTRA_FLAGS} install --overwrite -y -j${SLURM_CPUS_PER_TASK} --log-format=junit --log-file=${CI_PROJECT_DIR}/install.xml --keep-stage ${SPACK_FULL_SPEC} | tee log) || install_failed=1
    - set +o pipefail
    # Then extract the hash from the log - FIXME this may be very brittle
    - SPACK_INSTALLED_HASH=$(spack find --format "{hash}" /$(sed -ne '${s/.*-//;p}' log))
    - echo "Determined the installed hash to be ${SPACK_INSTALLED_HASH}"
    # Construct the directories Spack is going to use to build the package.
    # For a CMake project the source directory will be:
    #   ${SPACK_STAGE_DIR}/spack-src
    # and the build directoy will by default be
    #   ${SPACK_STAGE_DIR}/spack-build-{short_hash}
    # TODO: to improve ccache support, drop the {short_hash} part. This needs a
    # change to `cmake.py` in Spack.
    - SPACK_STAGE_DIR=${SPACK_BUILD}/spack-stage-${SPACK_PACKAGE}-develop-${SPACK_INSTALLED_HASH}
    - SPACK_BUILD_DIR=${SPACK_STAGE_DIR}/spack-build-${SPACK_INSTALLED_HASH:0:7}
    - SPACK_SOURCE_DIR=${SPACK_STAGE_DIR}/spack-src
    # Try and improve debuggability, Spack likes making things only-owner-readable.
    - chmod -R g+rX "${SPACK_BUILD}"
    # Exit if we failed just above.
    - if [[ ${install_failed} == 1 ]]; then exit 1; fi
    - if [ ${SPACK_USE_CCACHE+x} ]; then
    # Report ccache statistics if ccache was enabled.
    - ccache --cleanup
    - ccache --show-stats --verbose
    # Create the cache archive for GitLab to handle. Intentionally do not compress (again) here.
    - tar -C "${CCACHE_DIR}" -cf "${CI_PROJECT_DIR}/ccache.tar" .
    - fi
    # Copy some files to the original working directory that artifacts are
    # saved from.  Make sure that optional files exist.
    - touch ${SPACK_STAGE_DIR}/spack-configure-args.txt
    - cp ${SPACK_STAGE_DIR}/spack-{build-env,build-out,configure-args}.txt ${CI_PROJECT_DIR}/
    # Overwrite the spack_build_info.env file, otherwise chains of dependent
    # build jobs will duplicate these values.
    - echo "SPACK_BUILD_DIR=${SPACK_BUILD_DIR}" > ${CI_PROJECT_DIR}/spack_build_info.env
    - echo "SPACK_FULL_SPEC=${SPACK_FULL_SPEC}" >> ${CI_PROJECT_DIR}/spack_build_info.env
    - echo "SPACK_SOURCE_DIR=${SPACK_SOURCE_DIR}" >> ${CI_PROJECT_DIR}/spack_build_info.env
    - echo "SPACK_INSTALLED_HASH=${SPACK_INSTALLED_HASH}" >> ${CI_PROJECT_DIR}/spack_build_info.env
    - SPACK_PACKAGE_SLUGIFY=$(echo -n ${SPACK_PACKAGE} | tr -c '[:alnum:]' '_' | tr '[:lower:]' '[:upper:]')
    - echo "${SPACK_PACKAGE_SLUGIFY}_INSTALLED_HASH=${SPACK_INSTALLED_HASH}" >> ${CI_PROJECT_DIR}/spack_build_info.env
    - echo "SPACK_PACKAGE_DEPENDENCY_ON_PREVIOUS_JOB=^/${SPACK_INSTALLED_HASH}" >> ${CI_PROJECT_DIR}/spack_build_info.env
    # Return a meaningful status code by inspecting install.xml. This only
    # loads modules in a subshell so it doesn't pollute the environment. The
    # snippet counts the number of <failure> tags in the XML.
    - num_failures=$(module load unstable python-dev; python -c "from lxml import etree; xml = etree.parse('${CI_PROJECT_DIR}/install.xml'); print(sum(1 for _ in xml.getroot().iter('failure')) + sum(1 for _ in xml.getroot().iter('error')))")
    - if [[ ${num_failures} > 0 ]]; then exit ${num_failures}; fi
  needs: ["spack_setup"]
  artifacts:
    when: always
    paths:
      - install.xml
      - spack_build_info.env
      - spack_clone_variables.env
      - initial_environment.env
      - spack-build-env.txt
      - spack-build-out.txt
      - spack-configure-args.txt
    reports:
      junit: install.xml
      dotenv:
        - spack_build_info.env
        - spack_clone_variables.env
  cache:
    key: ${CI_JOB_NAME}
    paths: [ccache.tar]
    policy: pull-push

.spack_test:
  extends: .gitlab_pipelines_variables
  stage: test
  variables:
    # Just running tests, no need to check anything out
    GIT_STRATEGY: none
    # Run tests with many 1-core tasks instead of 1 many-core task, as this
    # means that naive `mpirun -n ...` should work.
    bb5_ntasks: 8
    bb5_cpus_per_task: 1
  before_script:
    # Dump the environment for debugging purposes
    - env -0 | sort -z | xargs -0 -L 1 echo > initial_environment.env
    # Tell CTest to use the available slots. This may not quite be correct if
    # the tasks are allocated across several nodes.
    - export CTEST_PARALLEL_LEVEL=${SLURM_TASKS_PER_NODE}
    # Load the Spack installation that knows about the package to be tested.
    - . ${SPACK_ROOT}/share/spack/setup-env.sh
  script:
    - spack load /${SPACK_INSTALLED_HASH}
    - sh ${SPACK_SOURCE_DIR}/.ci/test_${CI_JOB_NAME}.sh
  artifacts:
    when: always
    paths:
      - initial_environment.env

.ctest:
  extends: .spack_test
  script:
    # Change to the build directory of the package being tested. This is
    # somewhere under the working directory of a previous job in the pipeline.
    - cd ${SPACK_BUILD_DIR}
    # Yuck, but otherwise boost unit tests output colour codes as part of
    # ctest -VV and the XML translation fails.
    - export BOOST_TEST_COLOR_OUTPUT=no
    # Make sure we return a helpful exit code.
    - i_am_a_failure=0
    # --output-on-failure should stop the output from parallel jobs being
    # interleaved. The full logfile including output from all jobs is uploaded
    # as an artefact, so we shouldn't lose any information.
    - spack build-env ${SPACK_FULL_SPEC} -- ctest --no-tests=error --output-on-failure -T Test || i_am_a_failure=1
    # Save the Testing/ directory as job artifacts
    - cp -r Testing/ ${CI_PROJECT_DIR}/
    # Make an XML report file the GitLab UI can display
    - module load unstable unit-test-translator
    - cmake2junit > ${CI_PROJECT_DIR}/ctest.xml
    - exit ${i_am_a_failure}
  artifacts:
    when: always
    paths:
      # Ugly to re-declare this from .spack_test, but oh well.
      - initial_environment.env
      - Testing/
    reports:
      junit: ctest.xml
